## PAU: Design and Implementation of Posit-Based Vector Arithmetic Unit for Low-Power Platforms
### Abstract:
With the rapid development of edge computing, artificial intelligence, and other fields, the traditional IEEE 754 floating-point system faces precision and efficiency bottlenecks, particularly exhibiting high energy consumption and precision loss in resource-constrained scenarios. The Posit number system, with its adaptive precision, broader dynamic range, and low hardware consumption, has become a research hotspot, especially suited for edge computing. However, existing research primarily focuses on scalar computations, which are insufficient to meet the demands of large-scale parallel data processing. This paper presents, for the first time, a Posit Vector Arithmetic Unit (PVU) designed using the Chisel language, supporting vector operations such as addition, subtraction, multiplication, division, and dot product, thus overcoming the limitations of traditional scalar designs. By integrating customized RISC-V instruction extensions, this design significantly improves computational performance and energy efficiency. The contributions of this paper include the efficient implementation of the vector arithmetic unit, parameterized and modular hardware design, support for custom RISC-V instructions, and practical application verification of the Posit number system, providing an innovative solution for the next-generation floating-point operations in edge computing.
**KEYWORDS**：IEEE 754、Posit、RISC-V、Accelerator、Arithmetic

在边缘计算、人工智能等领域的快速发展下，传统IEEE 754浮点数体系面临精度和效率瓶颈，尤其在资源受限场景中表现出高能耗和精度损失。Posit数制凭借自适应精度、更广动态范围和低硬件消耗成为研究热点，特别适用于边缘计算。然而，现有研究多集中于标量计算，难以满足大规模并行数据处理需求。本文首次提出基于Chisel语言设计的Posit向量运算单元（PVU），支持加法、减法、乘法、除法和点积等向量运算，突破了传统标量设计局限。结合定制化RISC-V指令扩展，该设计显著提升了计算性能和能效比。本文贡献包括高效的向量运算单元实现、参数化和模块化硬件设计、自定义RISC-V指令支持及Posit数制实际应用验证，为下一代浮点数运算在边缘计算中的应用提供了创新解决方案。

### Introduction：
With the continuous rise of Artificial Intelligence of Things (AIoT), the demands for numerical computation precision and computational efficiency in edge computing are constantly increasing. While the traditional IEEE 754 floating-point system performs stably in most scenarios, its fixed precision allocation and rounding mechanisms have gradually revealed issues such as lack of flexibility, high energy consumption, and significant precision loss in certain specialized applications. To address these challenges, the Posit number system, proposed by John L. Gustafson in 2017, has gradually become one of the research hotspots in both academia and industry due to its advantages of adaptive precision distribution, wider dynamic range, and lower hardware resource consumption.

However, research on Posit floating-point arithmetic hardware is limited, and most studies adopt a scalar computation model, where data is processed sequentially. While this design is relatively simple and the implementation is more mature, it tends to create performance bottlenecks when handling large-scale parallel data processing. In contrast, vector arithmetic units, by performing parallel operations on multiple data elements simultaneously, significantly improve data throughput and computational efficiency. In fields such as signal processing, image processing, scientific computing, and machine learning, vector arithmetic units have demonstrated higher energy efficiency and lower power consumption, becoming a key breakthrough for solving complex computational tasks.

The RISC-V Instruction Set Architecture (ISA) is an open and extensible architecture designed to meet the diverse application needs ranging from embedded systems to high-performance computing. RISC-V stands out for its simplicity, efficiency, modularity, and strong flexibility. Its openness allows anyone to use and extend it for free, providing a significant advantage over architectures like X86 and ARM. The RISC-V Vector Extension (RVV) is designed to enhance processor performance in data-parallel tasks by supporting vector operations, enabling the processor to perform parallel computations on multiple data elements within a single instruction cycle. This greatly improves data processing efficiency in fields such as scientific computing, image processing, and machine learning.

Chisel, as a modern hardware construction language based on Scala, provides an ideal platform for rapid prototyping and verification of hardware architectures due to its high-level abstraction, modular design, and flexible hardware description capabilities. Using Chisel for hardware design not only significantly enhances development efficiency but also facilitates higher levels of design reuse and parameterized optimization.

For the reasons mentioned above, this study aims to design and implement a Posit Vector Arithmetic Unit (PVU) based on the Chisel language. The unit supports operations such as addition, subtraction, multiplication, division, and vector dot products. Additionally, by utilizing customized RISC-V instruction extensions, it breaks the limitations of traditional scalar designs and provides a novel solution for next-generation high-performance, low-power computing platforms. Specifically, the main contributions of this work are as follows:
- Efficient Implementation of the Vector Arithmetic Unit: The Posit Vector Arithmetic Unit designed in this study can simultaneously process multiple sets of data in parallel, supporting five operations: vector addition, vector subtraction, vector multiplication, vector division, and vector dot product. This design overcomes the performance bottlenecks of traditional scalar computation models in large-scale data processing.
- Parameterized and Modular Hardware Design: Using the high-level abstraction and parameterization capabilities of the Chisel language, this study constructs a modular and scalable hardware architecture. The Posit bit width, exponent bit width, and mantissa alignment bit width are parameterized, allowing for flexible configuration and expansion in different application scenarios. This design approach significantly reduces development costs and provides a solid foundation for subsequent system-level integration and functional extensions.
- Custom RISC-V Instruction Support: To fully leverage the performance advantages of the Posit Vector Arithmetic Unit, this study customizes a series of Posit vector operation instructions based on the RISC-V Vector Extension (RVV). These custom instructions allow direct invocation of the vector arithmetic unit for data processing, optimizing the data transfer and instruction decoding processes. This significantly enhances the collaboration efficiency between hardware and software, thereby achieving higher overall system computational performance and energy efficiency.
- Posit Number System Application Verification: Through experimental and simulation results, it is demonstrated that the vector arithmetic unit exhibits strong competitiveness in terms of precision, speed, and energy consumption. Furthermore, in most application scenarios, the Posit number system offers higher computational precision compared to the IEEE 754 floating-point system, while also showing significant advantages in reducing power consumption and resource usage. This verifies its potential for application in high-performance, low-power computing platforms.
The structure of the remaining parts of this paper is as follows: Section 2 introduces the Posit number system and the RISC-V instruction set. Section 3 compares the similarities and differences between this study and other existing research. Section 4 provides a detailed description of the logical implementation of the PVU, with a focus on the multiplication module. Section 5 discusses the custom instruction set architecture extensions and compilation support. Section 6 describes the hardware cost and computational precision. The final section presents the conclusion.

### Blackground：
The Posit number system improves the efficiency and precision of numerical representation by introducing a flexible encoding mechanism. As shown in Figure 1, its basic components include Sign, Regime, Exponent, and Fraction. Each component has a unique function and structure, allowing the Posit number system to achieve a balance between precision and range. The following is a detailed introduction to each component:
- The Sign bit (S) is the first bit in the Posit number system and is responsible for determining the sign of the Posit number. If \( s = 0 \), Posit is positive; if \( s = 1 \), Posit is negative.
- The Regime field (R) is one of the key innovations of the Posit number system. It uses a variable-length encoding scheme to represent the range of the exponent, determining the scale of the Posit. The Regime is composed of consecutive 1s and 0s, with the length controlled by the inversion bit \( R_0 \). When the Posit is large, the Regime will contain more 1s; conversely, when the Posit is smaller, the Regime will contain more 0s. The length of the Regime field is \( k \), and the value \( r \) is calculated using the following formula:

- The Regime value is a special constant scaling factor \( U_{\text{seed}} \), and its value depends on the configuration of the Posit. Specifically, it is determined by the bit width \( E_S \) of the Exponent part in the Posit. The calculation formula is as follows:

- The Exponent field (E) is the part of the Posit that represents the configured exponent. Since the Regime field has a variable length, the Exponent bits may appear after the least significant bit (LSB) of the Posit, in which case the value of the Exponent will be 0.
- The Fraction field (F) is the final part of the Posit number system, similar to the mantissa part in IEEE 754. However, there is always an implicit number \( m \) in front of the Fraction. When the Posit is positive, \( m = 1 \); when the Posit is negative, \( m = 2 \). This means that in actual storage, the mantissa part typically stores only the subsequent binary digits, with the first bit not needing to be explicitly stored. For example, the value \( 1.101 \) will store the bits \( 101 \) in the Fraction part, implicitly understood as \( 1.101 \).

Combining the four components mentioned above, although in the Posit Standard 2022, \( E_S \) is fixed at 2, for better evaluation, the PVU parameterizes \( E_S \), resulting in two calculation formulas. When \( E_S \neq 2 \), the Posit value \( p \) is calculated as shown in Equation (3), and when \( E_S = 2 \), the Posit value \( p \) is calculated as shown in Equation (4).

Specially, if the entire Posit file consists of 0s, the Posit value is 0. If the entire Posit file consists of 1s, the Posit value is Not a Real (NaR), which is the umbrella value for anything not mathematically definable as a unique real number.

For example, as shown in Figure 2, \( 0111110111101010 \) is the binary encoding of Posit<16,2>. The Sign bit of the Posit is 0, indicating that it is positive, so \( s = 0 \). The Regime field is \( 111110 \), consisting of 5 consecutive 1s and 1 0, with a length \( k = 6 \), thus \( r = 5 - 1 = 4 \). The Exponent field is \( 11 \), so \( e = 3 \), and the \( U_{\text{seed}} \) value is 16. Finally, the Fraction field is \( 1101010 \), so \( f = \frac{106}{2^7} = \frac{106}{128} = 0.828125 \). Therefore, using the formula (3), we can compute \( p = (1 + 0.828125) \times 2^{(1 - 2 \times 0)} \times (4 \times 4 + 2 + 0) = 239616 \).

### Related work：
In terms of the RISC-V architecture, the European PULP platform has explored low-power applications of the RISC-V architecture \cite{1}, particularly excelling in Internet of Things (IoT) devices and embedded systems. The advantage of RISC-V lies in its simple and highly modular instruction set, which can be extended according to specific needs, making it especially suitable for implementing customized designs on edge devices, such as accelerators dedicated to artificial intelligence inference tasks \cite{2}.

In the field of Posit floating-point software, SoftPosit is an open-source project developed by Berkeley Lab \cite{3}, which provides basic arithmetic support for Posit floating-point numbers and demonstrates the potential applications of Posit in high-precision numerical computations. The Posit library for Julia \cite{4} has implemented support for Posit floating-point numbers and is widely used in numerical computing and deep learning inference tasks. Research indicates that, under the same bit-width, Posit floating-point numbers significantly outperform IEEE 754 floating-point numbers in terms of computational precision and efficiency \cite{5}, especially in low-power, high-efficiency computing scenarios, where Posit has broad application prospects. There are also numerous studies in other fields that apply the Posit format, including image processing \cite{6}, weather forecasting \cite{7}, and more.

The team led by Florent de Dinechin at Lyon University in France explored the applicability of Posit floating-point numbers \cite{8} and found them to be highly suitable for the field of machine learning, demonstrating an exact conversion between Posit and IEEE 754 floating-point numbers. The team at Rochester Institute of Technology in the United States, led by Zachariah Carmichael, successfully ran deep neural networks (DNNs) on Posit floating-point numbers with less than 8 bits \cite{9}, achieving better accuracy and lower latency. The team at the University of Pisa in Italy, led by Marco Cococcioni, explored the potential for vectorized computation with Posit floating-point mechanisms through software simulation, integrating this into the DNN model training and inference process \cite{10}. The PeNSieve project at the Complutense University of Madrid proposed a Posit-based framework for training and inference of deep neural network models, incorporating low-bit quantization through fused operations \cite{11}.

Compared to the widespread software simulation and application of the Posit number system, research on hardware computation units is relatively scarce. The PERCIVAL \cite{12} and BIG-PERCIVAL \cite{13} projects at Complutense University of Madrid focus on the implementation of small-scale and large-scale Posit computations, aiming to accelerate deep learning inference and scientific computing. The PERC project \cite{14} at Aachen University of Technology in Germany studies how to integrate Posit operations into high-performance processors, while the PERI project \cite{15} at the Indian Institute of Technology Madras focuses on optimizing high-precision computation and energy efficiency for IoT devices. The POSAR project \cite{16} at the National University of Singapore designs lightweight Posit processing units for smart terminal devices and low-power applications, driving the widespread use of the RISC-V architecture in edge computing. The PPU-light project \cite{17} at the University of Pisa, Italy, enables the conversion of Posit numbers into other numeral systems. The team led by Li Qiong at Nanjing University has designed a configurable open-source Posit Dot Product Unit (PDPU) that can perform efficient dot product operations in deep learning applications and supports mixed precision \cite{18}. As shown in Table 1, we summarize some of these works and compare them with this study.

### Posit Vector Unit Arithmentic Operations And Implementation：
The architecture of the PVU is shown in Figure 3. The PVU takes two Posit vector operands (PV1, PV2) and an operation mode number (PV) as inputs, and outputs the corresponding Posit operation result. We have parameterized the design of the PVU, including the Posit bit width, Exponent bit width, and alignment width.

#### Decode：
After the Posit operands enter the PVU, they are first subject to vector decoding. First, we extract the sign bit. If the sign bit is 1, the Posit is negative; if the sign bit is 0, the Posit is positive. The extracted Posit binary is then modified to its two's complement representation.

Next, we calculate the bit width and value of the Regime. The value of the Regime part is computed using a Leading Zero Count (LZC) module. First, we extract the highest bit of the Regime part. If it is 0, the Regime is of the form (00....01), and we directly pass it to the LZC module for counting. If it is 1, the Regime is of the form (11....10), and we first invert all the binary digits before passing it to the LZC module for counting. The LZC module will return the number of leading zeros and whether it is all zeros. The former can be used with the highest bit of the Regime to substitute into Equation (1) to calculate the value of \( r \). Since the length of the Regime field is dynamic, there is a case where the Regime field occupies the entire Posit bit width. In this case, it can be determined using the latter condition.

The length of the Regime field, \( k \), can be obtained by adding 1 to the number of leading zeros. We then left-shift the Posit by \( k+1 \) bits using a barrel shifter, which allows us to extract the Exponent field from the highest bits. The bit width of the Exponent, \( E_S \), is determined by the Posit configuration (typically 2). If \( E_S \neq 2 \), we can substitute the \( E_S \) value into Equation (2) to calculate the \( U_{\text{seed}} \) value. If \( E_S = 2 \), no calculation of \( U_{\text{seed}} \) is needed. For ease of subsequent calculations, we combine the Regime and Exponent to calculate a unified binary exponent value, \( \text{exp} \). Specifically, we left-shift \( r \) by \( E_S \) bits and add the value of the Exponent field \( e \), yielding the value of \( \text{exp} \).

Finally, we left-shift the Posit by \( E_S \) bits using a barrel shifter to obtain the Fraction part. Note that in the Posit number system, the Fraction part has an implicit bit \( m \). We concatenate this implicit bit with the extracted Fraction to obtain the actual mantissa value used in the computation. At this point, we have successfully decoded the Posit and obtained the intermediate representation for Posit Intermediate Representation (PIR), which includes the sign indicating whether the Posit is positive or negative, the binary exponent value \( \text{exp} \), and the actual mantissa \( \text{frc} \).

#### Add/Sub：
After decoding, the computation phase begins, and all operations are performed based on PIR. Before performing addition and subtraction, the operands need to be aligned so that the exponents of the two operands are unified. Specifically, we first use a comparator to find the maximum exponent value, which will serve as the target value for alignment. We then calculate the difference between each exponent and the target, modify the exponent values, and shift the mantissa based on the exponent difference, ensuring that the actual mantissa value remains unchanged. It is important to note that the number of bits to be shifted cannot exceed the alignment value configured for the Posit, to ensure that the precision of the mantissa is not lost during the computation.

After the alignment is completed, the corresponding operation is performed based on the OP. Since the addition and subtraction operations share similar logic, we will introduce them together here. First, we need to check the sign bits of the two operands to determine their signs. Next, since the exponents of the two aligned operands are the same, the exponent result of the addition/subtraction operation is the exponent target from the alignment process. At this point, only the mantissas need to be added or subtracted, and the corresponding carry/borrow is recorded. Finally, the final sign is computed based on the carry/borrow values and the sign bits of the operands. At this stage, the sign, exponent, and mantissa values of the PIR are all calculated.

#### Mul：
The vector multiplication module is one of the core components of the PVU, and here we will focus on the implementation logic of this module. For multiplication, we do not need to perform the alignment process on the operands as we do for addition and subtraction. The multiplication module can directly process the decoded PIR vectors. First, the calculation of the sign bit is very simple; we can obtain the final sign bit by performing an XOR operation between the sign bits of the two operands.

Next, we compute the mantissa part, which is the most complex section of this module. To better utilize hardware parallelism and achieve faster and more accurate mantissa multiplication, we adopt the base-4 Booth multiplication algorithm in the design. Figure 4 illustrates our design logic. This algorithm effectively reduces the number of additions required for the multiplication operation by decomposing the multiplier into a base-4 encoded form. This results in higher speed and lower energy consumption, especially when handling large-scale data, and thus improves multiplication efficiency in hardware implementations compared to traditional multiplication algorithms. Specifically, we first pad the multiplier with zeros to align it to the format required by the signed Booth encoding, then perform the base-4 Booth encoding operation. The result is stored in the vector \texttt{codes}. Next, we decode the elements in \texttt{codes} sequentially, and according to the base-4 Booth multiplication algorithm, perform the appropriate calculations (such as bit inversion and left shifts, multiplication by 0, etc.). After the computation, the shifted results are concatenated to form a partial product.

Now, we need to perform the accumulation of the partial products. To improve the accumulation speed, we have designed a carry-save adder (CSA) tree unit using 4:2 compressors and 3:2 compressors. By utilizing a divide-and-conquer structure and recursive methods, we reduce the delay caused by carry propagation, thereby enhancing the overall performance of the addition process and efficiently handling large-scale accumulation operations. We input the partial products into the CSA, and the sum and carry are obtained. At this point, the implementation of the base-4 Booth multiplication algorithm is fully completed.

Finally, we perform the final summation of the sum and carry to complete the mantissa multiplication. The exponent calculation is much simpler; we only need to add the exponents of the two mantissas. It is worth noting that an overflow might occur after the summation, so we have set a maximum exponent to avoid this issue.

#### Div：
The difficulty in floating-point vector division lies in the handling of mantissa division. For the sign and exponent, the handling is the same as multiplication: the sign is determined through XOR computation. The exponent, on the other hand, can be calculated through integer subtraction.

In the mantissa division section, as shown in Equation (5), we convert the division operation into a multiplication operation and reuse the base-4 Booth multiplier for multiplication. At this point, the key issue is how to convert the integer into its corresponding reciprocal. To address this, we use Newton's method to compute the reciprocal, with the iteration formula shown in Equation (6). Here, \( X_n \) is the current approximation of the reciprocal, and \( \text{num} \) is the input integer. Through multiple iterations, \( X_n \) will converge to the approximation of \( \frac{1}{\text{num}} \). The more iterations performed, the higher the precision, but also the greater the delay. In the PVU, we set the number of iterations to 3.

Finally, we take the sum of the result and carry outputted from the base-4 Booth multiplier, and perform a shift operation to scale it back to the fixed-point format bit width.

#### Dot Product：
In deep learning, the frequency of dot product operations is very high, making it essential to support a dedicated dot product computation module in the PVU. As shown in Equation (6), we decompose the dot product calculation into two parts. The first part is the vector multiplication operation, where the result is stored in an intermediate vector. The second part is the accumulation operation, where the elements of the intermediate vector are summed to obtain the final dot product result.

First, we reuse the existing vector multiplication module, and store the result in an intermediate variable. Before performing the accumulation, we need to align all the elements within the intermediate variable. In the addition/subtraction module, the alignment operation is performed for two operands as a pair, while in dot product operations, the alignment must be applied to all elements within the intermediate vector. Therefore, we have developed an alignment module specifically adapted for dot product operations, with an internal implementation logic identical to that of the addition/subtraction alignment module.

After the alignment is completed, we convert all the mantissas to two's complement format based on the sign bits, and reuse the CSA for accumulation. Since the CSA operates with a larger bit width during computation and performs a rounding operation after all calculations are completed, this significantly improves the precision of the dot product computation.

#### Standardization：
As mentioned earlier, the Posit mantissa has an implicit bit. Before encoding the final result into Posit format, we need to normalize all the mantissas in the PIR. We treat the highest bit of the actual mantissa as the implicit bit and perform normalization to ensure that this bit is always 1.

We first invoke the LZC module to calculate the number of leading zeros in the mantissa. Then, using the number of leading zeros and the configured decimal point position, we calculate the adjustment required for the exponent. Finally, we use a barrel shifter to adjust the exponent so that the value before the decimal point is 1.

Finally, we save the mantissa bit width plus 1 bit, perform RNE rounding on the lower bits, and adjust the exponent values in the PIR one by one. It is important to note that the output of the dot product operation is a scalar, while the output of other operations is a vector. Therefore, the normalization module has both scalar and vector versions, but the internal logic is the same.

#### Encode：
After the computation is completed, we need to encode the various parts of the PIR into the specified Posit format. In fact, encoding is the inverse process of decoding, and we can handle it using the reverse process of decoding. The challenge lies in extracting the Regime part, as its dynamic bit width property means that the bit width of the Regime part after computation is unknown. Therefore, we first extract the Exponent part, which has a fixed bit width, and then extract the Regime part. In the Regime value computation part, to maintain the inversion bit property, we initialize the Regime value as 1 (00...01), then determine whether to invert it based on its sign. Finally, we calculate the Regime bit width inversely using the formula (1), completing the encoding of the Regime part.

Finally, we perform RNE rounding on the mantissa, concatenate the three parts, and convert them into two's complement representation to complete the Posit encoding. Note that since the result of the dot product operation is a scalar, we have also written a scalar encoding unit, which shares the same internal logic as the vector encoding unit.


### RISC-V ISA Extension And Compiler suppopt：
At the instruction set architecture level, we designed the necessary vector instructions for the PVU using the RISC-V V extension. As shown in Table 2, we have customized corresponding instructions for the operations ADD, SUB, MUL, DIV, and DOT. Since the Posit number system is intended to replace the traditional IEEE 754 floating-point type, the custom instruction format follows the OPFVV instruction format. We use the custom encoding space \( 001101 \) to replace the original func6 field to indicate that the instruction is for Posit number system operations, and differentiate the various Posit operations using the fun3 field. For the vector mask, we simplified \( V_m \) to 1, indicating the use of a vector mask.

To support the use of custom instructions by hardware, we wrote a set of functions using inline assembly to map high-level C/C++ function calls to low-level machine code. At this point, we do not need to modify any RISC-V compilers, as the correct data flow for Posit instructions is generated during the compilation process.

We present an example of inline assembly for vector multiplication in Listing 1. Here, we assume each element is 32 bits, and each operation processes 4 elements at a time. The register keyword allows the compiler to place the input and output vectors in the specified registers and set the vector length. In the inline assembly, lines 9, 10, and 11 are where we set the OP opcode, funct3 field, and funct6 field for specific Posit operations, which are the key parts distinguishing them from other instructions and operations.

Convolution is a commonly used operation in deep learning. In Listing 2, we present an example algorithm for performing a 4×4 convolution using the PVU. We vectorize the convolution kernel by rows, loading both the rows and columns into the computation vector in one step. The result is computed all at once using vector multiplication and stored in the result vector. Finally, the result is summed using vector addition to complete the convolution computation.

### Experimental Results：
To evaluate the computational correctness of each operation module in the PVU, we extracted the quantized activation values and weights from the first convolution layer of ResNet-18 \cite{19} using PyTorch, and saved them in FP64 format. Since the PVU operates using the Posit number system, we used the SoftPosit library to batch-convert the extracted data to Posit32 and perform addition, subtraction, multiplication, division, and dot product calculations. This framework provided the necessary inputs and outputs for testing. We wrote the Kconfig framework and conducted tests, where the PVU achieved 100% accuracy in vector addition, vector subtraction, vector multiplication, and vector dot product operations. The accuracy for vector division was 95.84%, which is due to the errors introduced by the reciprocal conversion.

In addition, we tested the accuracy difference between the Posit number system and FP32 in DNNs using Deep PeNSieve \cite{20}. As shown in Figures 6 and 7, we conducted tests on MNIST, Fashion-MNIST, SVHN, and CIFAR-10 at the TOP-1 and TOP-5 levels. It can be observed that Posit16 achieves higher accuracy than FP32 while reducing memory usage by half.

We also performed FPGA testing of the PVU using Vavido, as shown in Table 3......

### Conclusion：
In this paper, we propose an open-source Posit Vector Arithmetic Unit (PVU) that can efficiently perform addition, subtraction, multiplication, division, and dot product operations in deep learning applications on low-power platforms. The PVU can be tightly integrated with RISC-V and, through inline assembly, map high-level languages to machine code. Additionally, a configurable generator for the PVU has been developed to support different Posit number systems in various low-power scenarios.


---
[1] Pullini A, Rossi D, Loi I, Tagliavini G, Benini L. Mr.Wolf: An Energy-Precision Scalable Parallel Ultra Low Power SoC for IoT Edge Processing[J]. IEEE Journal of Solid-State Circuits, 2019, 54(7): 1970-1981. DOI:10.1109/JSSC.2019.2912307.
[2] Wang S, Wang X, Xu Z, Chen B, Feng C, Wang Q, Ye T T. Optimizing CNN Computation Using RISC-V Custom Instruction Sets for Edge Platforms[J]. IEEE Transactions on Computers, 2024, 73(5): 1371-1382.
[3] C. Leong, “Softposit,” https://gitlab.com/cerlane/SoftPosit, 2018.
[4] Klöwer M, Düben PD, Palmer TN. SoftPosit.jl: A posit arithmetic emulator for Julia[EB/OL]. GitHub repository, 2020.
[5] S. D. Ciocirlan, D. Loghin, L. Ramapantulu, N. Tapus, and Y. M. Teo, “The Accuracy and Efficiency ofPosit Arithmetic,” 2021, arXiv:2109.08225.
[6]N. Shah, L. I. G. Olascoaga, S. Zhao, W. Meert, and M. Verhelst, “DPU:
DAG processing unit for irregular graphs with precision-scalable posit
arithmetic in 28 nm,” IEEE J. Solid State Circuits (JSSC), vol. 57, no. 8,
pp. 2586–2596, 2022.
[7]N. Ho, D. T. Nguyen, H. D. Silva, J. L. Gustafson, W. Wong, and I. J.
Chang, “Posit arithmetic for the training and deployment of generative
adversarial networks,” in 2021 Design, Automation & Test in Europe
Conference & Exhibition (DATE). IEEE, 2021, pp. 1350–1355.
[8] de Dinechin, F., Forget, L., Muller, J.-M., & Uguen, Y. Posits: The Good, the Bad and the Ugly. Univ Lyon, INSA Lyon, Inria, CITI, Lyon, France.
[9] Carmichael, Z., Langroudi, H. F., Khazanov, C., Lillie, J., Gustafson, J. L., & Kudithipudi, D. Deep Positron: A Deep Neural Network Using the Posit Number System. Neuromorphic AI Lab, Rochester Institute of Technology, NY, USA, National University of Singapore, Singapore.
[10] Cococcioni, M., Rossi, F., Ruffaldi, E., & Saponara, S. Fast Approximations of Activation Functions in Deep Neural Networks when using Posit Arithmetic. _Department of Information Engineering, Università di Pisa_, Medical Microinstruments (MMI) S.p.A., 2020. Published: March 10, 2020.
[11] Murillo, R., Del Barrio, A. A., & Botella, G. (2020). Deep PeNSieve: A deep learning framework based on the posit number system. _Digital Signal Processing_, 102, 102762.
[12] Mallas, D., Murillo, R., Del Barrio, A. A., Botella, G., Piñuel, L., & Prieto-Matias, M. PERCIVAL: Open-Source Posit RISC-V Core With Quire Capability[J]. IEEE Access, Vol. 10, No. 3, July-Sept. 2022.
[13] Mallasén, D., Del Barrio, A. A., & Prieto-Matias, M. Big-PERCIVAL: Exploring the Native Use of 64-Bit Posit Arithmetic in Scientific Computing[EB/OL]. arXiv preprint, 2023, May.
[14] M. V. Arunkumar, S. G. Bhairathi, and H. G. Hayatnagarkar, “PERC: Posit Enhanced Rocket Chip,” in 4th Workshop on Computer Architecture Research with RISC-V (CARRV’20), 2020, p. 8.
[15] S. Tiwari, N. Gala, C. Rebeiro, and V. Kamakoti, “PERI: A Configurable Posit Enabled RISC-V Core,” ACM Transactions on Architecture and Code Optimization, vol. 18, no. 3, pp. 1–26, Jun. 2021.
[16] S. D. Ciocirlan, D. Loghin, L. Ramapantulu, N. Tapus, and Y. M. Teo, “The Accuracy and Efficiency of Posit Arithmetic,” 2021, arXiv:2109.08225.
[17] M. Cococcioni, F. Rossi, E. Ruffaldi, and S. Saponara, “A Lightweight Posit Processing Unit for RISC-V Processors in Deep Neural Network Applications,” IEEE Transactions on Emerging Topics in Computing, no. 01, pp. 1–1, Oct. 2021.
[18] Li, Q., Fang, C., & Wang, Z. PDPU: An Open-Source Posit Dot-Product Unit for Deep Learning Applications. In _2023 IEEE International Symposium on Circuits and Systems (ISCAS)_, 2023. IEEE.
[19]K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, pp. 770–778.
[20]Murillo, Raul, Alberto A. Del Barrio, and Guillermo Botella. "Deep PeNSieve: A Deep Learning Framework Based on the Posit Number System." _Digital Signal Processing_ 102 (2020): 102762. doi:10.1016/j.dsp.2020.102762.
